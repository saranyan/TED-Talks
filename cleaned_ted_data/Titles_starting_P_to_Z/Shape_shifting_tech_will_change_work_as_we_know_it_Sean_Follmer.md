
We&#39;ve evolved with tools,
and tools have evolved with us.
Our ancestors created these
hand axes 1.5 million years ago,
shaping them to not only
fit the task at hand
but also their hand.
However, over the years,
tools have become
more and more specialized.
These sculpting tools
have evolved through their use,
and each one has a different form
which matches its function.
And they leverage
the dexterity of our hands
in order to manipulate things
with much more precision.
But as tools have become
more and more complex,
we need more complex controls
to control them.
And so designers have become
very adept at creating interfaces
that allow you to manipulate parameters
while you&#39;re attending to other things,
such as taking a photograph
and changing the focus
or the aperture.
But the computer has fundamentally
changed the way we think about tools
because computation is dynamic.
So it can do a million different things
and run a million different applications.
However, computers have
the same static physical form
for all of these different applications
and the same static
interface elements as well.
And I believe that this
is fundamentally a problem,
because it doesn&#39;t really allow us
to interact with our hands
and capture the rich dexterity
that we have in our bodies.
And my belief is that, then,
we must need new types of interfaces
that can capture these
rich abilities that we have
and that can physically adapt to us
and allow us to interact in new ways.
And so that&#39;s what I&#39;ve been doing
at the MIT Media Lab
and now at Stanford.
So with my colleagues,
Daniel Leithinger and Hiroshi Ishii,
we created inFORM,
where the interface can actually
come off the screen
and you can physically manipulate it.
Or you can visualize
3D information physically
and touch it and feel it
to understand it in new ways.
Or you can interact through gestures
and direct deformations
to sculpt digital clay.
Or interface elements can arise
out of the surface
and change on demand.
And the idea is that for each
individual application,
the physical form can be matched
to the application.
And I believe this represents a new way
that we can interact with information,
by making it physical.
So the question is, how can we use this?
Traditionally, urban planners
and architects build physical models
of cities and buildings
to better understand them.
So with Tony Tang at the Media Lab,
we created an interface built on inFORM
to allow urban planners
to design and view entire cities.
And now you can walk around it,
but it&#39;s dynamic, it&#39;s physical,
and you can also interact directly.
Or you can look at different views,
such as population or traffic information,
but it&#39;s made physical.
We also believe that these dynamic
shape displays can really change
the ways that we remotely
collaborate with people.
So when we&#39;re working together in person,
I&#39;m not only looking at your face
but I&#39;m also gesturing
and manipulating objects,
and that&#39;s really hard to do
when you&#39;re using tools like Skype.
And so using inFORM,
you can reach out from the screen
and manipulate things at a distance.
So we used the pins of the display
to represent people&#39;s hands,
allowing them to actually touch
and manipulate objects at a distance.
And you can also manipulate
and collaborate on 3D data sets as well,
so you can gesture around them
as well as manipulate them.
And that allows people to collaborate
on these new types of 3D information
in a richer way than might
be possible with traditional tools.
And so you can also
bring in existing objects,
and those will be captured on one side
and transmitted to the other.
Or you can have an object that&#39;s linked
between two places,
so as I move a ball on one side,
the ball moves on the other as well.
And so we do this by capturing
the remote user
using a depth-sensing camera
like a Microsoft Kinect.
Now, you might be wondering
how does this all work,
and essentially, what it is,
is 900 linear actuators
that are connected to these
mechanical linkages
that allow motion down here
to be propagated in these pins above.
So it&#39;s not that complex
compared to what&#39;s going on at CERN,
but it did take a long time
for us to build it.
And so we started with a single motor,
a single linear actuator,
and then we had to design
a custom circuit board to control them.
And then we had to make a lot of them.
And so the problem with having
900 of something
is that you have to do
every step 900 times.
And so that meant that we had
a lot of work to do.
So we sort of set up
a mini-sweatshop in the Media Lab
and brought undergrads in and convinced
them to do &quot;research&quot; --

(Laughter)

and had late nights
watching movies, eating pizza
and screwing in thousands of screws.
You know -- research.

(Laughter)

But anyway, I think that we were
really excited by the things
that inFORM allowed us to do.
Increasingly, we&#39;re using mobile devices,
and we interact on the go.
But mobile devices, just like computers,
are used for so many
different applications.
So you use them to talk on the phone,
to surf the web, to play games,
to take pictures
or even a million different things.
But again, they have the same
static physical form
for each of these applications.
And so we wanted to know how can we take
some of the same interactions
that we developed for inFORM
and bring them to mobile devices.
So at Stanford, we created
this haptic edge display,
which is a mobile device
with an array of linear actuators
that can change shape,
so you can feel in your hand
where you are as you&#39;re reading a book.
Or you can feel in your pocket
new types of tactile sensations
that are richer than the vibration.
Or buttons can emerge from the side
that allow you to interact
where you want them to be.
Or you can play games
and have actual buttons.
And so we were able to do this
by embedding 40 small, tiny
linear actuators inside the device,
and that allow you not only to touch them
but also back-drive them as well.
But we&#39;ve also looked at other ways
to create more complex shape change.
So we&#39;ve used pneumatic actuation
to create a morphing device
where you can go from something
that looks a lot like a phone ...
to a wristband on the go.
And so together with Ken Nakagaki
at the Media Lab,
we created this new
high-resolution version
that uses an array of servomotors
to change from interactive wristband
to a touch-input device
to a phone.

(Laughter)

And we&#39;re also interested
in looking at ways
that users can actually
deform the interfaces
to shape them into the devices
that they want to use.
So you can make something
like a game controller,
and then the system will understand
what shape it&#39;s in
and change to that mode.
So, where does this point?
How do we move forward from here?
I think, really, where we are today
is in this new age
of the Internet of Things,
where we have computers everywhere --
they&#39;re in our pockets,
they&#39;re in our walls,
they&#39;re in almost every device
that you&#39;ll buy in the next five years.
But what if we stopped
thinking about devices
and think instead about environments?
And so how can we have smart furniture
or smart rooms or smart environments
or cities that can adapt to us physically,
and allow us to do new ways
of collaborating with people
and doing new types of tasks?
So for the Milan Design Week,
we created TRANSFORM,
which is an interactive table-scale
version of these shape displays,
which can move physical objects
on the surface; for example,
reminding you to take your keys.
But it can also transform
to fit different ways of interacting.
So if you want to work,
then it can change to sort of
set up your work system.
And so as you bring a device over,
it creates all the affordances you need
and brings other objects
to help you accomplish those goals.
So, in conclusion,
I really think that we need to think
about a new, fundamentally different way
of interacting with computers.
We need computers
that can physically adapt to us
and adapt to the ways
that we want to use them
and really harness the rich dexterity
that we have of our hands,
and our ability to think spatially
about information by making it physical.
But looking forward, I think we need
to go beyond this, beyond devices,
to really think about new ways
that we can bring people together,
and bring our information into the world,
and think about smart environments
that can adapt to us physically.
So with that, I will leave you.
Thank you very much.

(Applause)

