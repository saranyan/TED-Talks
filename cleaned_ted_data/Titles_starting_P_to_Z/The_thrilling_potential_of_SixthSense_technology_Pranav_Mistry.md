
We grew up
interacting with the physical
objects around us.
There are an enormous number of them
that we use every day.
Unlike most of our computing devices,
these objects are much more fun to use.
When you talk about objects,
one other thing automatically
comes attached to that thing,

and that is gestures:
how we manipulate these objects,
how we use these objects in everyday life.
We use gestures not only to interact
with these objects,
but we also use them
to interact with each other.
A gesture of &quot;Namaste!&quot;,
maybe, to respect someone, or maybe,
in India I don&#39;t need to teach
a kid that this means
&quot;four runs&quot; in cricket.
It comes as a part
of our everyday learning.
So, I am very interested,
from the beginning,
how our knowledge
about everyday objects and gestures,
and how we use these objects,
can be leveraged to our interactions
with the digital world.
Rather than using a keyboard and mouse,
why can I not use my computer
in the same way that I interact
in the physical world?
So, I started this exploration
around eight years back,
and it literally started
with a mouse on my desk.
Rather than using it for my computer,
I actually opened it.
Most of you might be aware
that, in those days,
the mouse used to come with a ball inside,
and there were two rollers
that actually guide the computer
where the ball is moving,
and, accordingly,
where the mouse is moving.
So, I was interested in these two rollers,
and I actually wanted more, so I borrowed
another mouse from a friend --
never returned to him --
and I now had four rollers.
Interestingly, what I did
with these rollers is,
basically, I took them off of these mouses
and then put them in one line.
It had some strings
and pulleys and some springs.
What I got is basically
a gesture-interface device
that actually acts
as a motion-sensing device
made for two dollars.
So, here, whatever movement
I do in my physical world
is actually replicated
inside the digital world
just using this small device
that I made, around eight years back,
in 2000.
Because I was interested
in integrating these two worlds,
I thought of sticky notes.
I thought, &quot;Why can I not connect
the normal interface
of a physical sticky note
to the digital world?&quot;
A message written
on a sticky note to my mom,
on paper,
can come to an SMS,
or maybe a meeting reminder
automatically syncs
with my digital calendar --
a to-do list that automatically
syncs with you.
But you can also search
in the digital world,
or maybe you can write a query, saying,
&quot;What is Dr. Smith&#39;s address?&quot;
and this small system
actually prints it out --
so it actually acts like a paper
input-output system,
just made out of paper.
In another exploration,
I thought of making a pen
that can draw in three dimensions.
So, I implemented this pen
that can help designers and architects
not only think in three dimensions,
but they can actually draw,
so that it&#39;s more intuitive
to use that way.
Then I thought,
&quot;Why not make a Google Map,
but in the physical world?&quot;
Rather than typing a keyword
to find something,
I put my objects on top of it.
If I put a boarding pass, it will show me
where the flight gate is.
A coffee cup will show
where you can find more coffee,
or where you can trash the cup.
So, these were some
of the earlier explorations I did
because the goal was to connect
these two worlds seamlessly.
Among all these experiments,

there was one thing in common:
I was trying to bring
a part of the physical world
to the digital world.
I was taking some part of the objects,
or any of the intuitiveness of real life,
and bringing them to the digital world,
because the goal was to make
our computing interfaces more intuitive.
But then I realized that we humans
are not actually interested in computing.
What we are interested in is information.
We want to know about things.
We want to know about
dynamic things going around.
So I thought, around last year --
in the beginning of the last year --
I started thinking, &quot;Why can I not take
this approach in the reverse way?&quot;
Maybe, &quot;How about I take my digital world
and paint the physical world
with that digital information?&quot;
Because pixels are actually, right now,
confined in these rectangular devices
that fit in our pockets.
Why can I not remove this confine
and take that to my everyday
objects, everyday life
so that I don&#39;t need
to learn the new language
for interacting with those pixels?
So, in order to realize this dream,
I actually thought of putting
a big-size projector on my head.
I think that&#39;s why this is called
a head-mounted projector, isn&#39;t it?
I took it very literally,
and took my bike helmet,
put a little cut over there so that
the projector actually fits nicely.
So now, what I can do --
I can augment the world around me
with this digital information.
But later,
I realized that I actually
wanted to interact
with those digital pixels, also.
So I put a small camera over there
that acts as a digital eye.
Later, we moved to a much better,
consumer-oriented pendant version of that,
that many of you now know
as the SixthSense device.
But the most interesting thing
about this particular technology
is that you can carry
your digital world with you
wherever you go.
You can start using any surface,
any wall around you,
as an interface.
The camera is actually tracking
all your gestures.
Whatever you&#39;re doing with your hands,
it&#39;s understanding that gesture.
And, actually, if you see,
there are some color markers
that in the beginning version
we are using with it.
You can start painting on any wall.
You stop by a wall,
and start painting on that wall.
But we are not only tracking
one finger, here.
We are giving you the freedom
of using all of both of your hands,
so you can actually use both of your hands
to zoom into or zoom out
of a map just by pinching all present.
The camera is actually doing --
just, getting all the images --
is doing the edge recognition
and also the color recognition
and so many other small algorithms
are going on inside.
So, technically,
it&#39;s a little bit complex,
but it gives you an output which is more
intuitive to use, in some sense.
But I&#39;m more excited that you can
actually take it outside.
Rather than getting your camera
out of your pocket,
you can just do the gesture
of taking a photo,
and it takes a photo for you.

(Applause)

Thank you.
And later I can find a wall, anywhere,
and start browsing those photos
or maybe, &quot;OK, I want to modify
this photo a little bit
and send it as an email to a friend.&quot;
So, we are looking for an era
where computing will actually merge
with the physical world.
And, of course,
if you don&#39;t have any surface,
you can start using your palm
for simple operations.
Here, I&#39;m dialing a phone number
just using my hand.
The camera is actually not
only understanding your hand movements,
but, interestingly,
is also able to understand what objects
you are holding in your hand.
For example, in this case,
the book cover is matched
with so many thousands,
or maybe millions of books online,
and checking out which book it is.
Once it has that information,
it finds out more reviews about that,
or maybe New York Times
has a sound overview on that,
so you can actually hear,
on a physical book,
a review as sound.
(Video) Famous talk
at Harvard University --
This was Obama&#39;s visit last week to MIT.
(Video) And particularly I want
to thank two outstanding MIT --

Pranav Mistry: So, I was seeing
the live [video] of his talk,
outside, on just a newspaper.
Your newspaper will show you
live weather information
rather than having it updated.
You have to check your computer
in order to do that, right?

(Applause)

When I&#39;m going back,
I can just use my boarding pass
to check how much my flight
has been delayed,
because at that particular time,
I&#39;m not feeling like opening my iPhone,
and checking out a particular icon.
And I think this technology
will not only change the way --

(Laughter)

Yes.
It will change the way
we interact with people, also,
not only the physical world.
The fun part is, I&#39;m going
to the Boston metro,
and playing a pong game inside the train
on the ground, right?

(Laughter)

And I think the imagination
is the only limit
of what you can think of
when this kind of technology
merges with real life.
But many of you argue, actually,
that all of our work is not
only about physical objects.
We actually do lots
of accounting and paper editing
and all those kinds of things;
what about that?
And many of you are excited
about the next-generation tablet computers
to come out in the market.
So, rather than waiting for that,
I actually made my own,
just using a piece of paper.
So, what I did here
is remove the camera --
All the webcam cameras have
a microphone inside the camera.
I removed the microphone from that,
and then just pinched that --
like I just made a clip
out of the microphone --
and clipped that to a piece of paper,
any paper that you found around.
So now the sound of the touch
is getting me when exactly
I&#39;m touching the paper.
But the camera is actually tracking
where my fingers are moving.
You can of course watch movies.
(Video) Good afternoon.
My name is Russell,
and I am a Wilderness
Explorer in Tribe 54.&quot;

PM: And you can of course play games.
(Car engine)
Here, the camera is actually understanding
how you&#39;re holding the paper
and playing a car-racing game.

(Applause)

Many of you already must have
thought, OK, you can browse.
Yeah. Of course you can
browse to any websites
or you can do all sorts
of computing on a piece of paper
wherever you need it.
So, more interestingly,
I&#39;m interested in how we can
take that in a more dynamic way.
When I come back to my desk,
I can just pinch that information
back to my desktop
so I can use my full-size computer.

(Applause)

And why only computers?
We can just play with papers.
Paper world is interesting to play with.
Here, I&#39;m taking a part of a document,
and putting over here a second part
from a second place,
and I&#39;m actually modifying the information
that I have over there.
Yeah. And I say, &quot;OK, this looks nice,
let me print it out, that thing.&quot;
So I now have a print-out of that thing.
So the workflow is more intuitive,
the way we used to do it
maybe 20 years back,
rather than now switching
between these two worlds.
So, as a last thought,
I think that integrating
information to everyday objects
will not only help us to get rid
of the digital divide,
the gap between these two worlds,
but will also help us, in some way,
to stay human,
to be more connected
to our physical world.
And it will actually help us
not end up being machines
sitting in front of other machines.
That&#39;s all. Thank you.

(Applause)

Thank you.

(Applause)


Chris Anderson: So, Pranav,
first of all, you&#39;re a genius.
This is incredible, really.
What are you doing with this?
Is there a company being planned?
Or is this research forever, or what?

Pranav Mistry: So, there are
lots of companies,
sponsor companies of Media Lab
interested in taking this ahead
in one or another way.
Companies like mobile-phone operators
want to take this in a different way
than the NGOs in India,
thinking, &quot;Why can we only
have &#39;Sixth Sense&#39;?
We should have a &#39;Fifth Sense&#39;
for missing-sense people who cannot speak.
This technology can be used for them
to speak out in a different way
maybe a speaker system.&quot;

CA: What are your own plans?
Are you staying at MIT,
or are you going to do
something with this?

PM: I&#39;m trying to make this
more available to people
so that anyone can develop
their own SixthSense device,
because the hardware is actually
not that hard to manufacture
or hard to make your own.
We will provide all the open source
software for them,
maybe starting next month.

CA: Open source? Wow.

(Applause)


CA: Are you going to come back to India
with some of this, at some point?

PM: Yeah. Yes, yes, of course.

CA: What are your plans? MIT? India?
How are you going to split
your time going forward?

PM: There is a lot of energy here.
Lots of learning.
All of this work that you have seen
is all about my learning in India.
And now, if you see, it&#39;s more about

the cost-effectiveness:
this system costs you $300
compared to the $20,000 surface tables,
or anything like that.
Or maybe even the $2 mouse gesture system
at that time was costing around $5,000?
I showed that, at a conference,
to President Abdul Kalam, at that time,
and then he said, &quot;OK, we should use this
in Bhabha Atomic Research Centre
for some use of that.&quot;
So I&#39;m excited about how I can bring
the technology to the masses
rather than just keeping that technology
in the lab environment.

(Applause)


CA: Based on the people we&#39;ve seen at TED,
I would say you&#39;re truly
one of the two or three
best inventors in the world right now.
It&#39;s an honor to have you at TED.
Thank you so much.
That&#39;s fantastic.

(Applause)

