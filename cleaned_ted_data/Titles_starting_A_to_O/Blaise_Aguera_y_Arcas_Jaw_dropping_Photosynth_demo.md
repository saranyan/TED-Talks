
What I&#39;m going to show you first,
as quickly as I can,
is some foundational work,
some new technology
that we brought to Microsoft
as part of an acquisition
almost exactly a year ago.
This is Seadragon, and it&#39;s an environment
in which you can either
locally or remotely interact
with vast amounts of visual data.
We&#39;re looking at many, many gigabytes
of digital photos here
and kind of seamlessly
and continuously zooming in,
panning through it,
rearranging it in any way we want.
And it doesn&#39;t matter how much
information we&#39;re looking at,
how big these collections are
or how big the images are.
Most of them are ordinary
digital camera photos,
but this one, for example,
is a scan from the Library of Congress,
and it&#39;s in the 300 megapixel range.
It doesn&#39;t make any difference
because the only thing that ought to limit
the performance of a system like this one
is the number of pixels on your screen
at any given moment.
It&#39;s also very flexible architecture.
This is an entire book,
so this is an example of non-image data.
This is &quot;Bleak House&quot; by Dickens.
Every column is a chapter.
To prove to you that it&#39;s really text,
and not an image,
we can do something
like so, to really show
that this is a real representation
of the text; it&#39;s not a picture.
Maybe this is an artificial way
to read an e-book.
I wouldn&#39;t recommend it.
This is a more realistic case,
an issue of The Guardian.
Every large image
is the beginning of a section.
And this really gives you
the joy and the good experience
of reading the real paper version
of a magazine or a newspaper,
which is an inherently
multi-scale kind of medium.
We&#39;ve done something
with the corner of this particular
issue of The Guardian.
We&#39;ve made up a fake ad
that&#39;s very high resolution --
much higher than in an ordinary ad --
and we&#39;ve embedded extra content.
If you want to see the features
of this car, you can see it here.
Or other models,
or even technical specifications.
And this really gets
at some of these ideas
about really doing away
with those limits on screen real estate.
We hope that this means no more pop-ups
and other rubbish like that --
shouldn&#39;t be necessary.
Of course, mapping is one
of those obvious applications
for a technology like this.
And this one I really
won&#39;t spend any time on,
except to say that we have things
to contribute to this field as well.
But those are all the roads in the U.S.
superimposed on top
of a NASA geospatial image.
So let&#39;s pull up, now, something else.
This is actually live on the Web now;
you can go check it out.
This is a project called Photosynth,
which marries two different technologies.
One of them is Seadragon
and the other is some very
beautiful computer-vision research
done by Noah Snavely, a graduate student
at the University of Washington,
co-advised by Steve Seitz at U.W.
and Rick Szeliski at Microsoft Research.
A very nice collaboration.
And so this is live on the Web.
It&#39;s powered by Seadragon.
You can see that
when we do these sorts of views,
where we can dive through images
and have this kind
of multi-resolution experience.
But the spatial arrangement of the images
here is actually meaningful.
The computer vision algorithms
have registered these images together
so that they correspond to the real
space in which these shots --
all taken near Grassi Lakes
in the Canadian Rockies --
all these shots were taken.
So you see elements here
of stabilized slide-show
or panoramic imaging,
and these things have
all been related spatially.
I&#39;m not sure if I have time
to show you any other environments.
Some are much more spatial.
I would like to jump straight
to one of Noah&#39;s original data-sets --
this is from an early prototype
that we first got working this summer --
to show you what I think
is really the punch line
behind the Photosynth technology,
It&#39;s not necessarily so apparent
from looking at the environments
we&#39;ve put up on the website.
We had to worry
about the lawyers and so on.
This is a reconstruction
of Notre Dame Cathedral
that was done entirely computationally
from images scraped from Flickr.
You just type Notre Dame into Flickr,
and you get some pictures of guys
in T-shirts, and of the campus and so on.
And each of these orange cones
represents an image
that was discovered
to belong to this model.
And so these are all Flickr images,
and they&#39;ve all been related
spatially in this way.
We can just navigate
in this very simple way.

(Applause)

(Applause ends)
You know, I never thought
that I&#39;d end up working at Microsoft.
It&#39;s very gratifying to have
this kind of reception here.

(Laughter)

I guess you can see this is

lots of different types of cameras:
it&#39;s everything from cell-phone cameras
to professional SLRs,
quite a large number of them,
stitched together in this environment.
If I can find some
of the sort of weird ones --
So many of them are occluded
by faces, and so on.
Somewhere in here there is actually
a series of photographs -- here we go.
This is actually a poster of Notre Dame
that registered correctly.
We can dive in from the poster
to a physical view of this environment.
What the point here really is
is that we can do things
with the social environment.
This is now taking data from everybody --
from the entire collective memory,
visually, of what the Earth looks like --
and link all of that together.
Those photos become linked,
and they make something emergent
that&#39;s greater than the sum of the parts.
You have a model that emerges
of the entire Earth.
Think of this as the long tail
to Stephen Lawler&#39;s Virtual Earth work.
And this is something that grows
in complexity as people use it,
and whose benefits become greater
to the users as they use it.
Their own photos are getting tagged
with meta-data that somebody else entered.
If somebody bothered
to tag all of these saints
and say who they all are,
then my photo of Notre Dame Cathedral
suddenly gets enriched
with all of that data,
and I can use it as an entry point
to dive into that space,
into that meta-verse,
using everybody else&#39;s photos,
and do a kind of a cross-modal
and cross-user social experience that way.
And of course, a by-product of all of that
is immensely rich virtual models
of every interesting part of the Earth,
collected not just from overhead flights
and from satellite images
and so on, but from the collective memory.
Thank you so much.

(Applause)

(Applause ends)

Chris Anderson:
Do I understand this right?
What your software is going to allow,
is that at some point,
really within the next few years,
all the pictures that are shared
by anyone across the world
are going to link together?

BAA: Yes. What this is really
doing is discovering,
creating hyperlinks,
if you will, between images.
It&#39;s doing that based on the content
inside the images.
And that gets really exciting
when you think about the richness
of the semantic information
a lot of images have.
Like when you do a web search for images,
you type in phrases,
and the text on the web page is carrying
a lot of information
about what that picture is of.
What if that picture links
to all of your pictures?
The amount of semantic
interconnection and richness
that comes out of that is really huge.
It&#39;s a classic network effect.

CA: Truly incredible. Congratulations.
